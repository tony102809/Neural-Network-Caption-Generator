{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lOeBxsdSTROC"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional; nn.f = functional\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms\n",
        "import string\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch options\n",
        "batch_size = 128  # input batch size for training\n",
        "test_batch = 1    # test batch size\n",
        "\n",
        "# Normalizes the input images for a better training\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "# Get train and test loaders to load MNIST data\n",
        "trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
        "testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
        "\n",
        "# Create the dataloaders on which we can iterate to get the data in batch_size chunks\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader  = torch.utils.data.DataLoader(testset, batch_size=test_batch, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "ETgYHEFoT0Yj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelDict = None\n",
        "vocab = None\n",
        "# Creating dictionary and Lookup Table ###########################################\n",
        "labelDict = {\n",
        "    0: \"<b>zero<e>\",\n",
        "    1: \"<b>one<e>\",\n",
        "    2: \"<b>two<e>\",\n",
        "    3: \"<b>three<e>\",\n",
        "    4: \"<b>four<e>\",\n",
        "    5: \"<b>five<e>\",\n",
        "    6: \"<b>six<e>\",\n",
        "    7: \"<b>seven<e>\",\n",
        "    8: \"<b>eight<e>\",\n",
        "    9: \"<b>nine<e>\"\n",
        "}\n",
        "characters = set()\n",
        "for word in labelDict.values():\n",
        "    word_clean = word[3:-3]  # Strip the boundary tokens\n",
        "    characters.update(word_clean)\n",
        "\n",
        "vocab = ['<b>', '<e>'] + sorted(characters)\n",
        "\n",
        "################################################################################\n",
        "vocab.sort()\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)\n",
        "\n",
        "# Testing\n",
        "assert(vocab_size == 17)\n",
        "assert(len(labelDict) == 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAhYFGB3UDj3",
        "outputId": "3894ede3-2075-4823-d3a4-3f96dedc3d41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<b>', '<e>', 'e', 'f', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't', 'u', 'v', 'w', 'x', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some utility functions to check if code is working as intended\n",
        "def assert_encoding(actual, expected):\n",
        "  assert(type(actual) == type(torch.tensor(0)))\n",
        "  if(not((expected.numpy() == actual.numpy()).all())):\n",
        "    print('expected: ', expected)\n",
        "    print('actual: ', actual)\n",
        "    assert((expected.numpy() == actual.numpy()).all())\n",
        "\n",
        "# Utility function to assert shape of a tensor, for debugging purpose\n",
        "def shapeChecker(x, y):\n",
        "  assert(x.shape == y.shape)\n",
        "\n",
        "# Get the index of a token in the vocab\n",
        "def get_idx(letter):\n",
        "    return [i for i, x in enumerate(vocab) if (letter == x)][0]"
      ],
      "metadata": {
        "id": "QHzA8fucUFZz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a list of labels to a list of one-hot matrix\n",
        "def label_to_onehot_sequence(label):\n",
        "    # Retrieve the tokenized string for the given label from labelDict\n",
        "    tokenized_string = labelDict[label.item()]\n",
        "\n",
        "    # Handle the tokens properly, treating '<b>' and '<e>' as single tokens\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokenized_string):\n",
        "        if tokenized_string[i] == '<':  # Detect the start of a special token\n",
        "            end_idx = tokenized_string.find('>', i + 1)\n",
        "            tokens.append(tokenized_string[i:end_idx + 1])\n",
        "            i = end_idx + 1\n",
        "        else:\n",
        "            tokens.append(tokenized_string[i])\n",
        "            i += 1\n",
        "\n",
        "    # Convert the tokens into a list of indices based on the vocab\n",
        "    indices = [vocab.index(token) for token in tokens]\n",
        "\n",
        "    # Convert list of indices to a tensor\n",
        "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "    # Generate a one-hot encoded tensor from the indices_tensor using nn.f.one_hot\n",
        "    one_hot_encoded = nn.f.one_hot(indices_tensor, num_classes=len(vocab))\n",
        "\n",
        "    return one_hot_encoded\n",
        "\n",
        "\n",
        "assert_encoding(label_to_onehot_sequence(torch.tensor(3)),\n",
        "  torch.tensor([[\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   # <b>\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],   #  t\n",
        "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  h\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],   #  r\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], # <e>\n",
        "    dtype=torch.float32))"
      ],
      "metadata": {
        "id": "-iXxtMXcUIRG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_of_labels_to_onehot_matrix(labels):\n",
        "    # Convert labels to one-hot tensors\n",
        "    onehot_inputs = [label_to_onehot_sequence(label) for label in labels]\n",
        "\n",
        "    # Pad the length of string since, matrix operation requires fixed-size rows\n",
        "    max_len = max(len(onehot) for onehot in onehot_inputs)\n",
        "    padded_onehot = pad_sequence(onehot_inputs, batch_first=True, padding_value= 0)\n",
        "    return max_len, padded_onehot\n",
        "\n",
        "# Convert label to label onehot - used in the next to feed argmax input to RNN\n",
        "def label_to_onehot(target):\n",
        "    return nn.f.one_hot(target, num_classes=10).to(torch.float32)\n"
      ],
      "metadata": {
        "id": "OK6uaB0AUVxb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert token indices predicted by our model back to characters and form the word\n",
        "def token_idx_to_token(input):\n",
        "\n",
        "    # Convert list of token idx to '<b>φ<e>' ##################################\n",
        "    result = ''.join(vocab[idx] for idx in input)\n",
        "    return result\n",
        "    # For each index in the input list, get the corresponding token from vocab, to build the output word\n",
        "    # example input -> [ 0, 3, 6, 13, 2,  1] -> '<b>five<e>'\n",
        "    #                    ⬇ ⬇ ⬇   ⬇ ⬇  ⬇\n",
        "    #                   <b> f  i   v  e  <e>\n",
        "    return None\n",
        "\n",
        "assert(token_idx_to_token([0, 3,  6, 13,  2,  1]) == '<b>five<e>')"
      ],
      "metadata": {
        "id": "IulSRHXcUZM2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_images(train_loader, num_images=10):\n",
        "    # Fetch a single batch of images\n",
        "    dataiter = iter(train_loader)\n",
        "    images, labels = dataiter.__next__()  # Use __next__() instead of next()\n",
        "\n",
        "    # Set up the matplotlib figure and axes\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))  # Adjust size as needed\n",
        "\n",
        "    # Display the first 'num_images' images\n",
        "    for i in range(num_images):\n",
        "        ax = axes[i]\n",
        "        img = images[i]  # Get the image tensor\n",
        "        label = labels[i].item()  # Get the label as a Python value\n",
        "\n",
        "        # Convert image tensor to numpy for display\n",
        "        npimg = img.numpy()\n",
        "        # Handle images with more than one color channel\n",
        "        if img.shape[0] > 1:\n",
        "            npimg = np.transpose(npimg, (1, 2, 0))  # Rearrange dimensions to (H, W, C)\n",
        "        else:\n",
        "            npimg = npimg.squeeze()  # Remove single-dimensional entries from the array\n",
        "\n",
        "        # Show the image\n",
        "        ax.imshow(npimg, cmap='gray')\n",
        "        # Set the title to the token sequence from labelDict\n",
        "        ax.set_title(f\"{labelDict[label]}\")\n",
        "        ax.axis('off')  # Hide axes ticks\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to visualize the images along with the label\n",
        "visualise_images(train_loader)  # Assuming `train_loader` is your DataLoader object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "WtF5f79cUcCk",
        "outputId": "8b51bbec-a8b1-48a5-e85e-c20c6a8a3aec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to print statistics for debugging as well as modifies target and output for accuracy calculation\n",
        "def trainUtility(batch_idx, epoch, output, target, padded_onehot):\n",
        "    # Printing statistics for easy debugging\n",
        "    if batch_idx == 1 or (epoch != None and epoch%500==0):\n",
        "        _, pred_idx = torch.max(output, dim=-1)\n",
        "        _, true_idx = torch.max(padded_onehot[:, 1:, :], dim=-1)\n",
        "        print('acc: ', torch.sum(pred_idx == true_idx)/(pred_idx.shape[0]*pred_idx.shape[1]))\n",
        "        print(pred_idx[0, :], true_idx[0, :])\n",
        "\n",
        "    # Converting output of size (batch_size x max_word_length-1 x vocab_size) -> ((batch_size*(max_word_length-1)) x vocab_size)\n",
        "    output =  output.view(-1, vocab_size)\n",
        "    # Converting the true labels to ((batch_size*(max_word_length-1)) x vocab_size)\n",
        "    # omitting the first character since, this is not getting predicted by the model\n",
        "    target = padded_onehot[:, 1:, :].reshape(-1, vocab_size)\n",
        "\n",
        "    return output, target"
      ],
      "metadata": {
        "id": "fkju4638UhPg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnTrainingDataset(Dataset):\n",
        "    \"\"\"Dataset to get a tuple of one-hot matrix for each label (batch_size x 10)\n",
        "       and corresponding one-hot matrix of words (batch_size x max_word_length - 1 x vocab_size)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        \"\"\"\n",
        "        self.X = label_to_onehot(torch.tensor([x for x in range(10)]))\n",
        "        _, self.Y = batch_of_labels_to_onehot_matrix(torch.tensor([x for x in range(10)]))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10 # There are only 10 words\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx], self.Y[idx])\n",
        "\n",
        "rnn_dataset = RnnTrainingDataset()\n",
        "rnn_dataloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=10, shuffle=True)\n",
        "rnn_testloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=1, shuffle=True)\n"
      ],
      "metadata": {
        "id": "1v2AMd7UUl8S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which does the training for number of epochs and model and type of model passed\n",
        "def train(num_epochs, optimiser, model, dataloader=train_loader, mode='ENCODER'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    lr_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "      for batch_idx, (data, target) in enumerate(dataloader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          optimiser.zero_grad()\n",
        "\n",
        "          loss = None\n",
        "          output = None\n",
        "\n",
        "\n",
        "          if(mode == 'ENCODER'): # CNN classifier\n",
        "                criterion = torch.nn.CrossEntropyLoss()\n",
        "                output = model(data)  # Forward pass: compute the model output\n",
        "                loss = criterion(output, target)  # Calculate loss: Cross entropy loss for classification\n",
        "                loss.backward()  # Backpropagation\n",
        "                optimiser.step()  # Optimization step\n",
        "\n",
        "\n",
        "          elif mode == 'DECODER':  # Overfitting RNN\n",
        "              # Process inputs and targets\n",
        "              input_sequence = target[:, :-1, :]  # Exclude the last token for prediction purposes\n",
        "              output = model(data, input_sequence)  # Generate output from the model\n",
        "\n",
        "              # Adjust the target to exclude the first token for training comparison\n",
        "              adjusted_target = target[:, 1:].argmax(dim=-1)  # Convert to class indices for loss calculation\n",
        "\n",
        "              # Ensure output and adjusted target have the same sequence length\n",
        "              if output.shape[1] != adjusted_target.shape[1]:\n",
        "                  # If not, it's likely the output is missing the last sequence element\n",
        "                  output = output[:, :adjusted_target.shape[1], :]\n",
        "\n",
        "              # Flatten output and adjusted target for loss calculation\n",
        "              output_flat = output.reshape(-1, output.shape[-1])\n",
        "              adjusted_target_flat = adjusted_target.reshape(-1)\n",
        "\n",
        "              # Calculate loss\n",
        "              loss = criterion(output_flat, adjusted_target_flat)\n",
        "              loss.backward()  # Backpropagate the loss\n",
        "              optimiser.step()  # Update the model parameters\n",
        "\n",
        "              print(\"Output flat shape:\", output_flat.shape)\n",
        "              print(\"Adjusted target flat shape:\", adjusted_target_flat.shape)\n",
        "\n",
        "\n",
        "          if (batch_idx+1) % 100 == 0 or (mode == 'DECODER' and epoch%500 == 0):\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, batch_idx+1, len(dataloader), loss.item()))"
      ],
      "metadata": {
        "id": "vkj7Jz9eUrCE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader=test_loader, mode='ENCODER'):\n",
        "    # Never forget to change model to eval mode using eval(), this freezes the model weights for updates\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in dataloader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        if(mode == 'ENCODER'):\n",
        "          # Predicted output\n",
        "          output = model(data)\n",
        "\n",
        "          # Calculate correct prediction count\n",
        "          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "          correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        elif(mode == 'DECODER'):\n",
        "          # Predicted output\n",
        "          output = torch.tensor(model.sample(data), device=device)\n",
        "\n",
        "          # True output\n",
        "          _, true = torch.max(target, dim=-1)\n",
        "          # Remove padding\n",
        "          true = torch.tensor(np.concatenate(([0],np.delete(true.cpu().numpy(),\n",
        "                                                            np.argwhere(true.cpu().numpy()==0)))), device=device)\n",
        "\n",
        "          # Calculate correct prediction count\n",
        "          correct += torch.sum(output == true)/output.shape[0]\n",
        "        elif(mode == 'MODULAR'):\n",
        "          # Predicted output\n",
        "          output = model.sample(data)\n",
        "\n",
        "          # True output\n",
        "          true = [labelDict[label.item()] for label in target]\n",
        "          true = torch.tensor([0] + [get_idx(c) for c in true[0]] + [1])\n",
        "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
        "\n",
        "          # Calculate correct prediction count\n",
        "          correct += torch.sum(output == true)/output.shape[0]\n",
        "        elif(mode == 'E2E'):\n",
        "          # Predicted output\n",
        "          output = torch.tensor(model.sample(data))\n",
        "\n",
        "          # True output\n",
        "          true = [labelDict[label.item()] for label in target]\n",
        "          true = torch.tensor([0] + [get_idx(c) for c in true[0]] + [1])\n",
        "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
        "\n",
        "          # Calculate correct prediction count\n",
        "          correct += torch.sum(output == true)/output.shape[0]\n",
        "\n",
        "    test_loss /= len(dataloader.dataset)\n",
        "    print('\\nTest set: Accuracy: {:.0f}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, len(dataloader.dataset),\n",
        "        100 * correct / len(dataloader.dataset)))\n",
        "\n",
        "    return 100 * correct / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "DhHwgT3FUwWr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# options\n",
        "epochs = 10         # number of epochs to train\n",
        "lr = 0.01           # learning rate\n",
        "\n",
        "cnnEncoder = None\n",
        "linearClassifier = None\n",
        "# Convolutional net architecture ###############################################\n",
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, img_size=28):  # Assuming MNIST image size by default\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Flatten the output for the linear layer\n",
        "        self.flatten_size = 16 * (img_size // 4 - 3) * (img_size // 4 - 3)\n",
        "        self.fc = nn.Linear(self.flatten_size, 84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Linear_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Linear_Classifier, self).__init__()\n",
        "        self.classifier = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "################################################################################\n",
        "cnnEncoder = CNN_Encoder()\n",
        "linearClassifier = Linear_Classifier()\n",
        "\n",
        "cnnClassifier = nn.Sequential(cnnEncoder, linearClassifier)\n",
        "enc_optimiser = optim.SGD(cnnClassifier.parameters(), lr=0.01)\n",
        "\n",
        "train(epochs, enc_optimiser, cnnClassifier, mode='ENCODER')\n",
        "acc = evaluate(cnnClassifier, test_loader, mode='ENCODER')\n",
        "print(f'Accuracy: {acc}%')\n",
        "assert(acc > 95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcLuLS_NU0Lt",
        "outputId": "af7fd602-541e-40b6-cecb-3d06193f83ca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/469], Loss: 1.9534\n",
            "Epoch [1/10], Step [200/469], Loss: 0.5896\n",
            "Epoch [1/10], Step [300/469], Loss: 0.3314\n",
            "Epoch [1/10], Step [400/469], Loss: 0.4160\n",
            "Epoch [2/10], Step [100/469], Loss: 0.2322\n",
            "Epoch [2/10], Step [200/469], Loss: 0.2181\n",
            "Epoch [2/10], Step [300/469], Loss: 0.2405\n",
            "Epoch [2/10], Step [400/469], Loss: 0.1499\n",
            "Epoch [3/10], Step [100/469], Loss: 0.1421\n",
            "Epoch [3/10], Step [200/469], Loss: 0.1777\n",
            "Epoch [3/10], Step [300/469], Loss: 0.1729\n",
            "Epoch [3/10], Step [400/469], Loss: 0.1113\n",
            "Epoch [4/10], Step [100/469], Loss: 0.1245\n",
            "Epoch [4/10], Step [200/469], Loss: 0.0615\n",
            "Epoch [4/10], Step [300/469], Loss: 0.1657\n",
            "Epoch [4/10], Step [400/469], Loss: 0.1737\n",
            "Epoch [5/10], Step [100/469], Loss: 0.0809\n",
            "Epoch [5/10], Step [200/469], Loss: 0.0613\n",
            "Epoch [5/10], Step [300/469], Loss: 0.1001\n",
            "Epoch [5/10], Step [400/469], Loss: 0.1250\n",
            "Epoch [6/10], Step [100/469], Loss: 0.0940\n",
            "Epoch [6/10], Step [200/469], Loss: 0.0706\n",
            "Epoch [6/10], Step [300/469], Loss: 0.1304\n",
            "Epoch [6/10], Step [400/469], Loss: 0.0558\n",
            "Epoch [7/10], Step [100/469], Loss: 0.0334\n",
            "Epoch [7/10], Step [200/469], Loss: 0.0684\n",
            "Epoch [7/10], Step [300/469], Loss: 0.0623\n",
            "Epoch [7/10], Step [400/469], Loss: 0.0912\n",
            "Epoch [8/10], Step [100/469], Loss: 0.0572\n",
            "Epoch [8/10], Step [200/469], Loss: 0.1152\n",
            "Epoch [8/10], Step [300/469], Loss: 0.0738\n",
            "Epoch [8/10], Step [400/469], Loss: 0.0614\n",
            "Epoch [9/10], Step [100/469], Loss: 0.0367\n",
            "Epoch [9/10], Step [200/469], Loss: 0.0213\n",
            "Epoch [9/10], Step [300/469], Loss: 0.1512\n",
            "Epoch [9/10], Step [400/469], Loss: 0.0694\n",
            "Epoch [10/10], Step [100/469], Loss: 0.0584\n",
            "Epoch [10/10], Step [200/469], Loss: 0.0825\n",
            "Epoch [10/10], Step [300/469], Loss: 0.0593\n",
            "Epoch [10/10], Step [400/469], Loss: 0.0755\n",
            "\n",
            "Test set: Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Accuracy: 97.94999694824219%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlQ6hhdnnOKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}